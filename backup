import sys
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_extract, col,expr
from pyspark.sql.types import DecimalType

##########################
# You can configure master here if you do not pass the spark.master paramenter in conf
##########################
#master = "spark://spark:7077"
#conf = SparkConf().setAppName("Spark Hello World").setMaster(master)
#sc = SparkContext(conf=conf)
#spark = SparkSession.builder.config(conf=conf).getOrCreate()

# Create spark context
spark = (SparkSession
    .builder
    .getOrCreate()
)
df = spark.read.format("csv").option("header","true").csv("/usr/local/spark/app/trips.csv")

result_origin_x = df.withColumn('origin_x', regexp_extract(col('origin_coord'), '([0-9]+.[0-9]+) ([0-9]+.[0-9]+)', 1))
output_origin_x_df = result_origin_x.withColumn("origin_x",result_origin_x["origin_x"].cast(DecimalType(4,2)))

result_origin_y = output_origin_x_df.withColumn('origin_y', regexp_extract(col('origin_coord'), '([0-9]+.[0-9]+) ([0-9]+.[0-9]+)', 2))
output_origin_y_df = result_origin_y.withColumn("origin_y",result_origin_y["origin_y"].cast(DecimalType(4,2)))

result_destination_x = output_origin_y_df.withColumn('dest_x', regexp_extract(col('destination_coord'), '([0-9]+.[0-9]+) ([0-9]+.[0-9]+)', 1))
output_destination_x_df = result_destination_x.withColumn("dest_x",result_destination_x["dest_x"].cast(DecimalType(4,2)))

result_destination_y = output_destination_x_df.withColumn('dest_y', regexp_extract(col('destination_coord'), '([0-9]+.[0-9]+) ([0-9]+.[0-9]+)', 2))
output_destination_y_df = result_destination_y.withColumn("dest_y",result_destination_y["dest_y"].cast(DecimalType(4,2)))


df.show()
####################################
# Load data to Postgres
####################################

print("######################################")
print("LOADING POSTGRES TABLES")
print("######################################")
(
    df.write
    .format("jdbc")
    .option("url", postgres_db)
    .option("dbtable", "public.trips_transformed")
    .option("user", postgres_user)
    .option("password", postgres_pwd)
    .save()
)
# output_destination_y_df.repartition(1).write.format("com.databricks.spark.csv").option("header", "true").save("/usr/local/spark/app/mydatad.csv")



# # Get the second argument passed to spark-submit (the first is the python app)
# logFile = sys.argv[1]

# # Read file
# logData = sc.textFile(logFile).cache()

# # Get lines with A
# numAs = logData.filter(lambda s: 'a' in s).count()

# # Get lines with B 
# numBs = logData.filter(lambda s: 'b' in s).count()

# # Print result
# print("Lines with a: {}, lines with b: {}".format(numAs, numBs))

